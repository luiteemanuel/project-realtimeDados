# Sistema de Processamento de Dados MeteorolÃ³gicos em Tempo Real

Sistema de ingestÃ£o, processamento e anÃ¡lise de dados meteorolÃ³gicos utilizando AWS, com capacidades de processamento em tempo real e em lote, incluindo sistema de alertas.

## ğŸ“‹ VisÃ£o Geral

Este projeto implementa uma arquitetura de dados completa na AWS para coletar dados meteorolÃ³gicos da API Tomorrow.io, processÃ¡-los em tempo real para alertas e armazenÃ¡-los para anÃ¡lise posterior atravÃ©s de pipelines ETL.

## ğŸ—ï¸ Arquitetura

A arquitetura Ã© dividida em trÃªs fluxos principais:

### 1. **IngestÃ£o de Dados**
- **API Gateway / CloudWatch Events** â†’ Aciona a funÃ§Ã£o Lambda `producer`
- **Lambda Producer** (`lambda_function.py`): 
  - Coleta dados meteorolÃ³gicos da API Tomorrow.io
  - Envia os dados para o Kinesis Data Streams (broker)

### 2. **Processamento em Tempo Real**
- **Kinesis Data Streams** (broker): Recebe os dados do producer
- **Lambda Consumer Realtime** (`consumer_realtime.py`):
  - Consome dados do Kinesis em tempo real
  - Monitora condiÃ§Ãµes meteorolÃ³gicas crÃ­ticas:
    - Probabilidade de chuva
    - Velocidade do vento
    - Rajada de vento
    - Intensidade da chuva
  - Envia alertas via **SNS** quando os limites configurados sÃ£o excedidos
  - SNS envia notificaÃ§Ãµes via SMS e e-mail

### 3. **Processamento em Lote (ETL)**
- **Lambda Consumer Batch** (`consumer_batch.py`):
  - Consome dados do Kinesis
  - Armazena dados brutos no bucket S3 `raw` com particionamento por data (year/month/day)
  
- **AWS Glue Crawler** (`raw_crawler`):
  - Varre o bucket S3 `raw`
  - Cria/atualiza tabelas no Glue Data Catalog (`raw_db`)

- **AWS Glue Job** (`weather_job` - `jobglue.py`):
  - Processa dados do `raw_db`
  - Transforma dados JSON em formato Parquet
  - Aplana estruturas aninhadas
  - Particiona dados por ano, mÃªs e dia
  - Armazena dados processados no bucket S3 `gold`

- **AWS Glue Crawler** (`gold_crawler`):
  - Varre o bucket S3 `gold`
  - Cria/atualiza tabelas no Glue Data Catalog (`gold_db`)

- **AWS Athena**: Consulta e anÃ¡lise dos dados processados no `gold_db`

## ğŸ“ Estrutura do Projeto

```
project-realtimeDados/
â”œâ”€â”€ lambda_function.py      # Producer Lambda - coleta dados da API
â”œâ”€â”€ consumer_realtime.py     # Consumer Lambda - processamento em tempo real e alertas
â”œâ”€â”€ consumer_batch.py        # Consumer Lambda - armazenamento em lote no S3
â”œâ”€â”€ jobglue.py              # AWS Glue Job - ETL e transformaÃ§Ã£o de dados
â”œâ”€â”€ requirements.txt        # DependÃªncias Python
â””â”€â”€ README.md              # Este arquivo
```

## ğŸ”§ Componentes

### Lambda Functions

#### `lambda_function.py` (Producer)
- **FunÃ§Ã£o**: Coleta dados meteorolÃ³gicos da API Tomorrow.io
- **Trigger**: API Gateway ou CloudWatch Events
- **SaÃ­da**: Kinesis Data Streams
- **VariÃ¡veis de Ambiente**:
  - `TOMORROW_API_KEY`: Chave da API Tomorrow.io

#### `consumer_realtime.py` (Consumer Realtime)
- **FunÃ§Ã£o**: Processa dados em tempo real e envia alertas
- **Trigger**: Kinesis Data Streams
- **SaÃ­da**: SNS (alertas)
- **VariÃ¡veis de Ambiente**:
  - `PRECIPITATION_PROBABILITY`: Limite de probabilidade de chuva (padrÃ£o: 10)
  - `WIND_SPEED`: Limite de velocidade do vento em m/s (padrÃ£o: 10)
  - `WIND_GUST`: Limite de rajada de vento em m/s (padrÃ£o: 10)
  - `RAIN_INTENSITY`: Limite de intensidade da chuva em mm/h (padrÃ£o: 10)

#### `consumer_batch.py` (Consumer Batch)
- **FunÃ§Ã£o**: Armazena dados brutos no S3
- **Trigger**: Kinesis Data Streams
- **SaÃ­da**: S3 bucket `raw`
- **VariÃ¡veis de Ambiente**:
  - `BUCKET_NAME`: Nome do bucket S3 para dados brutos

### AWS Glue Job

#### `jobglue.py` (Weather Job)
- **FunÃ§Ã£o**: ETL dos dados meteorolÃ³gicos
- **Entrada**: Glue Data Catalog (`raw_db`)
- **Processamento**:
  - Aplana estruturas JSON aninhadas
  - Extrai mÃ©tricas meteorolÃ³gicas (temperatura, umidade, vento, etc.)
  - Particiona dados por ano, mÃªs e dia
- **SaÃ­da**: S3 bucket `gold` em formato Parquet

## ğŸš€ ConfiguraÃ§Ã£o

### PrÃ©-requisitos

- Conta AWS configurada
- AWS CLI configurado
- Python 3.8+
- Credenciais AWS com permissÃµes apropriadas

### DependÃªncias

Instale as dependÃªncias:

```bash
pip install -r requirements.txt
```

### VariÃ¡veis de Ambiente

Configure as seguintes variÃ¡veis de ambiente nas suas funÃ§Ãµes Lambda:

**Producer Lambda:**
- `TOMORROW_API_KEY`: Sua chave da API Tomorrow.io

**Consumer Batch Lambda:**
- `BUCKET_NAME`: Nome do bucket S3 (ex: `weatherrt-bacth`)

**Consumer Realtime Lambda:**
- `PRECIPITATION_PROBABILITY`: Limite de probabilidade de chuva (%)
- `WIND_SPEED`: Limite de velocidade do vento (m/s)
- `WIND_GUST`: Limite de rajada de vento (m/s)
- `RAIN_INTENSITY`: Limite de intensidade da chuva (mm/h)

### Recursos AWS NecessÃ¡rios

1. **Kinesis Data Streams**: Stream chamado `broker`
2. **S3 Buckets**:
   - `raw`: Para dados brutos
   - `gold`: Para dados processados
3. **SNS Topic**: `snsalerta` (ARN: `arn:aws:sns:us-east-1:331104657282:snsalerta`)
4. **Glue Data Catalog**:
   - `raw_db`: Banco de dados para dados brutos
   - `gold_db`: Banco de dados para dados processados
5. **IAM Roles**:
   - `producer_iam`: PermissÃµes para Producer Lambda
   - `consumerrealtime_iam`: PermissÃµes para Consumer Realtime Lambda
   - `consumerbatch_iam`: PermissÃµes para Consumer Batch Lambda
   - `etl_role`: PermissÃµes para Glue Job

## ğŸ“Š Fluxo de Dados

```
API Tomorrow.io
    â†“
Producer Lambda
    â†“
Kinesis Data Streams (broker)
    â”œâ”€â”€â†’ Consumer Realtime Lambda â†’ SNS â†’ Alertas (SMS/E-mail)
    â””â”€â”€â†’ Consumer Batch Lambda â†’ S3 (raw)
            â†“
        Glue Crawler (raw_crawler)
            â†“
        Glue Data Catalog (raw_db)
            â†“
        Glue Job (weather_job)
            â†“
        S3 (gold)
            â†“
        Glue Crawler (gold_crawler)
            â†“
        Glue Data Catalog (gold_db)
            â†“
        AWS Athena (Consultas)
```

## ğŸ” PermissÃµes IAM

### Producer Lambda Role
- PermissÃ£o para escrever no Kinesis Data Streams

### Consumer Realtime Lambda Role
- PermissÃ£o para ler do Kinesis Data Streams
- PermissÃ£o para publicar no SNS

### Consumer Batch Lambda Role
- PermissÃ£o para ler do Kinesis Data Streams
- PermissÃ£o para escrever no bucket S3 `raw`

### Glue Job Role
- PermissÃ£o para ler do Glue Data Catalog
- PermissÃ£o para ler do bucket S3 `raw`
- PermissÃ£o para escrever no bucket S3 `gold`

## ğŸ“ Notas

- Os dados sÃ£o particionados por data (year/month/day) para otimizar consultas
- O formato Parquet Ã© usado na camada `gold` para melhor compressÃ£o e performance
- Os alertas sÃ£o enviados quando qualquer um dos limites configurados Ã© excedido
- A localizaÃ§Ã£o padrÃ£o configurada Ã©: Latitude -15.31227249, Longitude -49.11664409

## ğŸ”„ Melhorias Futuras

- [ ] Adicionar tratamento de erros mais robusto
- [ ] Implementar retry logic para chamadas de API
- [ ] Adicionar monitoramento com CloudWatch Metrics
- [ ] Implementar testes unitÃ¡rios
- [ ] Adicionar suporte a mÃºltiplas localizaÃ§Ãµes
- [ ] Implementar cache para reduzir chamadas Ã  API

## ğŸ“„ LicenÃ§a

Este projeto Ã© de uso interno.
